# PekoNet

This repository is the source code of [Improving Colloquial Case Legal Judgment Prediction via Abstractive Text Summarization](https://www.sciencedirect.com/science/article/abs/pii/S0267364923000730).

- [PekoNet](#pekonet)
  - [Introduction](#introduction)
  - [Environment and Requirements](#environment-and-requirements)
    - [Pretrained Language Models](#pretrained-language-models)
    - [Data](#data)
      - [Processed Data (Recommend)](#processed-data-recommend)
      - [Source Data](#source-data)
    - [Checkpoints](#checkpoints)
  - [Usage](#usage)
  - [Contact](#contact)
  - [Citation](#citation)

## Introduction
PekoNet is a colloquial case-based legal judgment prediction (LJP) framework based on abstractive text summarization. The main goal of the framework is to improve the performance of LJP on colloquial case facts and the experience for ordinary and non-professional users who use LJP services.

The framework is composed of two modules: Abstractive Text Summarization Module (ATSM) and Legal Judgment Prediction Module (LJPM). We first used a news summary dataset (CNewSum) to train ATSM. Then, we used ATSM to convert Taiwan criminal case facts from formal to colloquial. Last, we used colloquial case fact as the dataset to train LJPM.

<!-- In experiments, we prepared two datasets (30K Testing Set, 235 Testing Set) to compare the performance of the models (Orig. Model, Lead-3 Model, PekoNet) on the two prediction tasks (crime, article). -->

Here are the details of the datasets and models:
- datasets (Each dataset has two data formats: `Orig. Fact` and `Summary`)
    - TCI Training Set: A dataset there are 305,240 data, 147 charges, and 89 legal articles.
    - BART Testing Set: A dataset whose `Summary` data are generated by the BART model there are 30,289 data, 101 charges, and 89 legal articles.
    - ChatGPT Testing Set: A dataset whose `Summary` data are generated by the ChatGPT model there are 30,289 data, 101 charges, and 89 legal articles.
    - Human Testing Set: A dataset whose `Summary` data are written by humans there are 235 data, 53 charges, and 78 legal articles.
- models
  - `Baseline Model`: The PekoNet framework model without the ATS module, was trained by formal data of TCI Training Set.
  - `Independent Training Model`: The PekoNet framework model whose ATS module and LJP module are independent models, was trained by CNewSum and colloquial data of TCI Training Set.
  - `ATS-Freezing Model`: The PekoNet framework model whose ATS module was frozen when training LJP module, was trained by CNewSum and colloquial data of TCI Training Set.
  - `ATS-Finetuning Model`: The PekoNet framework model whose ATS module was finetuned when training LJP module, was trained by CNewSum and colloquial data of TCI Training Set.

<!-- Pic. 1. and 2. are the results of the experiments. We can see that PekoNet can improve the performance of prediction tasks (especially article prediction tasks) and has greater generality compared to other models.

![Micro_Results](https://drive.google.com/uc?export=view&id=1e9wPXn53oAAvUBeCmFDQn_hU_C9PSAUR)
> Pic. 1. Micro results

![Macro_Results](https://drive.google.com/uc?export=view&id=1eAljBGa89bn2NIJVFyy2l2r7iI6brobA)
> Pic. 2. Macro results -->

## Environment and Requirements
- Ubuntu 18.04.6 LTS
- Python 3.7.8
- OpenCC 1.1.4
- numpy 1.21.6
- scikit-learn 1.0.2
- tabulate 0.8.10
- torch 1.12.1
- tqdm 4.64.0
- transformers 4.21.2

<!-- ## Pretrain Language Models, Data and Checkpoints
The architecture of files may be:
```
root
| - models
| - results
|   | - tvt_dataset
|   | - checkpoints
|   ...
| - data
...
``` -->

### Pretrained Language Models
You should download and put the `models` folder in the root directory of this repository.
- [models](https://drive.google.com/drive/folders/1ggGjpeRzIX1C9DD6tlXMOa8mVsXE0bm0?usp=sharing)

### Data
#### Processed Data (Recommend)
These are the data we processed and used to train the model, and do experiments.
You should create the `results` folder in the root directory of this repository.
Then, download and put the `tvt_dataset` folder in `results`.
- [tvt_dataset (un-updated)]()

#### Source Data
These are the source data we used in this project.
If you want to generate all data by yourself, you should download and put the `data` folder in the root of this repository, and use `generate.py` to process data.
- [data (un-updated)]()

### Checkpoints
You should create the `results` folder in the root directory of this repository.
Then, download and put the `checkpoints` folder in `results`.
- [checkpoints (un-updated)]()

## Usage
> Unupdated.

<!-- ### main.py
> This file is used to train, evaluate, and test LJPM.

Basic Usage:
```
python ./main.py -c CONFIG -m MODE -g GPU [-cp CHECKPOINT_PATH] [-bs BATCH_SIZE] [-dt]
```
> You can use the `python ./main.py -h` command to see more information.

Example Usage:
```
python ./main.py -c ./configs/main/bart/one_label/config.ini -m train -g 0 -bs 6 -dt
```

### summarize.py
> This file is used to train, and test ATSM.

Basic Usage:
```
python ./summarize.py -c CONFIG -m MODE -g GPU [-cp CHECKPOINT_PATH] [-bs BATCH_SIZE]
```
> You can use the `python ./summarize.py -h` command to see more information.

Example Usage:
```
python ./summarize.py -c ./configs/summarize/CNewSum_v2/config.ini -m train -g 0 -bs 6
```

### generate.py
> This file is used to generate data (does not contain human labeling data).

Basic Usage:
```
python ./generate.py -c CONFIG [-g GPU] [-cp CHECKPOINT_PATH]
```
> You can use the `python ./generate.py -h` command to see more information.

Example Usage:
```
python ./generate.py -c ./configs/generate/legal_judgment_prediction/lead_3/one_label/config.ini
```

### convert.py
> This file is used to convert text from one Chinese type to another Chinese type by OpenCC.

Basic Usage:
```
python ./convert.py -sdp SOURCE_DIRECTORY_PATH -ddp DESTINATION_DIRECTORY_PATH -c CONFIG
```
> You can use the `python ./convert.py -h` command to see more information.
> The config of this file is the OpenCC config, check the config types of OpenCC to know how to use it.

Example Usage:
```
python ./convert.py -sdp ./origin -ddp ./converted -c s2t.json
```

### analyze.py
> This file is used to analyze and get the information of data.

Basic Usage:
```
python ./analyze.py -c CONFIG
```
> You can use the `python ./analyze.py -h` command to see more information.

Example Usage:
```
python ./analyze.py -c ./configs/analyze/taiwan_indictments/one_label/config.ini
``` -->

## Contact
If you have any questions, feel free to raise issues or contact me!

## Citation
```
@article{clsr2023hong,
    title = {Improving Colloquial Case Legal Judgment Prediction via Abstractive Text Summarization},
    journal = {Computer Law & Security Review},
    volume = {51},
    pages = {105863},
    year = {2023},
    issn = {0267-3649},
    doi = {https://doi.org/10.1016/j.clsr.2023.105863},
    url = {https://www.sciencedirect.com/science/article/pii/S0267364923000730},
    author = {Yu-Xiang Hong and Chia-Hui Chang},
    keywords = {Legal judgment prediction, Legal text summarization, Abstractive text summarization, Legal artificial intelligence}
}
```